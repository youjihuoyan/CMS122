{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcd12935",
   "metadata": {},
   "source": [
    "# CMT122 Generative Question Answering Training Using Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615ea761",
   "metadata": {},
   "source": [
    "## READ ME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758a932",
   "metadata": {},
   "source": [
    "This file requires an intact environment before running. Please run the first section of the code to verify the integrity of the entire environment before running it for the first time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "194bf878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ pandas loaded successfully.\n",
      "✅ Transformers loaded successfully\n",
      "✅ Datasets loaded successfully.\n",
      "✅ evaluate module loaded and metric initialized successfully.\n",
      "\n",
      "Environment check completed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "    print(\"Pandas loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Issue: {e}\")\n",
    "\n",
    "try:\n",
    "    from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "    print(\"Transformers loaded successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"Issue: {e}\")\n",
    "\n",
    "try:\n",
    "    from datasets import Dataset\n",
    "    print(\"Datasets loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Issue: {e}\")\n",
    "\n",
    "try:\n",
    "    import evaluate\n",
    "    print(\"Evaluate module loaded and metric initialized successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Evaluate module issue: {e}\")\n",
    "\n",
    "print(\"\\nEnvironment check completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9923563d-c3c4-42dc-8337-089860e9f402",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b1e407",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa48d5a",
   "metadata": {},
   "source": [
    "Read the data. This time the data will be fixed in UTF-8 encoding to avoid encoding problems that may affect the training input when reading the data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "560f4d59-bccf-4617-98b9-914bc9fecc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset value in utf-8 formate\n",
    "train_set = pd.read_csv('tweet_qa-train.csv',encoding = \"utf-8\")\n",
    "test_set = pd.read_csv('tweet_qa-test.csv',encoding = \"utf-8\")\n",
    "validation_set = pd.read_csv('tweet_qa-validation.csv',encoding = \"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a223511",
   "metadata": {},
   "source": [
    "Perform the first step of preprocessing on the overall training values, lowercase all English words to reduce noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f3f3eff-079f-46cb-8094-f8223faeab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lower case all letters\n",
    "columns_to_lowercase = ['text', 'context', 'gold_label_str']\n",
    "\n",
    "def lowercase(dataset, columns):\n",
    "    for column in columns:\n",
    "        if column in dataset.columns:\n",
    "            dataset[column] = dataset[column].str.lower()\n",
    "    return dataset\n",
    "\n",
    "train_set = lowercase(train_set, columns_to_lowercase)\n",
    "test_set = lowercase(test_set, columns_to_lowercase)\n",
    "validation_set = lowercase(validation_set, columns_to_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f573fc",
   "metadata": {},
   "source": [
    "Replace the incomprehensible names in the original list with questions and answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f4a48fa-477a-44c1-85ba-32a9a44ccd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns according to column_mapping, for easy understanding\n",
    "\n",
    "def rename_columns(dataset, column_mapping):\n",
    "    return dataset.rename(columns=column_mapping)\n",
    "\n",
    "column_mapping = {\n",
    "    'text': 'text',\n",
    "    'context': 'question',\n",
    "    'gold_label_str': 'answer'\n",
    "}\n",
    "\n",
    "train_set = rename_columns(train_set, column_mapping)\n",
    "test_set = rename_columns(test_set, column_mapping)\n",
    "validation_set = rename_columns(validation_set, column_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f978a2d7",
   "metadata": {},
   "source": [
    "Remove duplicates and missing items to avoid noise contamination of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d34c2af-561a-4fdc-8b0f-4563f9fa05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean up columns: remove duplicates, drop missing rows and ensure answers are found in text\n",
    "def dataset_cleaning(dataset, cleaning_columns):\n",
    "    dataset = dataset.drop_duplicates(subset=cleaning_columns)\n",
    "    dataset = dataset.dropna(subset=cleaning_columns)\n",
    "    return dataset\n",
    "\n",
    "cleaning_columns = ['text', 'question', 'answer']\n",
    "train_set_cleaned = dataset_cleaning(train_set, cleaning_columns)\n",
    "test_set_cleaned = dataset_cleaning(test_set, cleaning_columns)\n",
    "validation_set_cleaned = dataset_cleaning(validation_set, cleaning_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69beb3bb",
   "metadata": {},
   "source": [
    "Function to normalize column lengths to the average length of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e2c55e3-fac3-4ddc-af56-aa131eb4f3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize column lengths to the average length of each column\n",
    "def normalization_question_text(dataset, columns, padding_char=\" \"):\n",
    "    def adjust_length(value, target_length):\n",
    "        if len(value) > target_length:\n",
    "            return value[:target_length]\n",
    "        else:\n",
    "            return value + padding_char * (target_length - len(value))\n",
    "    for column in columns:\n",
    "        if column in dataset.columns:\n",
    "            avg_length = int(dataset[column].dropna().apply(len).mean())\n",
    "            dataset[column] = dataset[column].apply(\n",
    "                lambda x: adjust_length(x, avg_length) if isinstance(x, str) else x\n",
    "            )\n",
    "    return dataset\n",
    "train_set_normalized = normalization_question_text(train_set_cleaned, ['context', 'text'])\n",
    "test_set_normalized = normalization_question_text(test_set_cleaned, ['context', 'text'])\n",
    "validation_set_normalized = normalization_question_text(validation_set_cleaned, ['context', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709d71e7",
   "metadata": {},
   "source": [
    "Convert the dataset into Huggingface Dataset to facilitate subsequent model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b24fdc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '\"so much of the post is ben,\" mrs. graham said in 1994, three years after bradlee retired as editor. \"he created it as we know it today.\"— ed o\\'keefe (', 'question': 'what did bradlee retire as?', 'answer': 'editor', '__index_level_0__': 0}\n",
      "{'text': '5 years in 5 seconds. darren booth (@darbooth) january 25, 2013                                                                                          ', 'question': 'what site does the link take you to?', 'answer': 'vine', '__index_level_0__': 0}\n",
      "{'text': '\"@reid2962: @realdonaldtrump@foxnews i expected better from @megynkelly, wondering what is her hidden agenda.— donald j. trump (@realdonaldtrump) augus', 'question': 'who do you expect better from?', 'answer': '@megynkelly', '__index_level_0__': 0}\n"
     ]
    }
   ],
   "source": [
    "# Turn to Huggingface Dataset\n",
    "train_dataset = Dataset.from_pandas(train_set_normalized)\n",
    "validation_dataset = Dataset.from_pandas(test_set_normalized)\n",
    "test_dataset = Dataset.from_pandas(validation_set_normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087404d",
   "metadata": {},
   "source": [
    "Process the data into the data format used by the T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2eed87f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36f75dd192cb4ee385b29c7a33d53d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be7fbb78763c49e98a5444682263bfd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1d9b7e1f0bf4e168734b0f82127ad37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = [f\"question: {question} context: {text}\" for question, text in zip(examples['question'], examples['text'])]\n",
    "    targets = examples['answer']\n",
    "    return {'input_text': inputs, 'target_text': targets}\n",
    "\n",
    "train_dataset = train_dataset.map(preprocess_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(preprocess_function, batched=True)\n",
    "test_dataset = test_dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2634721f",
   "metadata": {},
   "source": [
    "This project uses the T5-small model for training. Due to the poor performance of the developer's computer, it is not possible to use the more professional base or large model for development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "57f4c0bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ff148cbc674dd8ac28e7db6d1393ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9449 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcedd7902a774ec3b267c04ed4f60193",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1197 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a49875cc08440f19518926846548d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['input_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    labels = tokenizer(examples['target_text'], padding=\"max_length\", truncation=True, max_length=128)\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "validation_dataset = validation_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dac262",
   "metadata": {},
   "source": [
    "The parameters of the model are set here, with the batch size set to 16 and the learning rate set to 2e-4, which are the highest values that the developer's host can run after adjustment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "277b4602",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Code\\Anaconda\\lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "#     fp16=True  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d7f6c663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2955' max='2955' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2955/2955 8:46:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.264400</td>\n",
       "      <td>0.051140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.054700</td>\n",
       "      <td>0.049074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.048641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.048855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.042700</td>\n",
       "      <td>0.048979</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2955, training_loss=0.08377779978382607, metrics={'train_runtime': 31599.3707, 'train_samples_per_second': 1.495, 'train_steps_per_second': 0.094, 'total_flos': 6394223410544640.0, 'train_loss': 0.08377779978382607, 'epoch': 5.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=validation_dataset,    \n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff48f687",
   "metadata": {},
   "source": [
    "Test the model first in the test set to avoid excessive deviation in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2e4ce31e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 02:57]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04575951769948006, 'eval_runtime': 180.3769, 'eval_samples_per_second': 6.015, 'eval_steps_per_second': 0.377, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(eval_dataset=test_dataset)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf03ecf8",
   "metadata": {},
   "source": [
    "Set the generation function to a fixed length of 128 and write the transcoded text to a file to facilitate subsequent developers to check the deviation of the generated content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59f5e4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4505d4f5d6c042a98e53c68d08deaee0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1085 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d2a838462b043a0bd1b83a1826965f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "5718969"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_answer(batch):\n",
    "    inputs = tokenizer(batch['input_text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model.generate(inputs.input_ids, max_length=128)\n",
    "    generated_answers = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs.input_ids,\n",
    "        max_length=50,\n",
    "        do_sample=True,\n",
    "        top_k=50,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    batch['generated_answer'] = generated_answers\n",
    "    return batch\n",
    "test_results = test_dataset.map(generate_answer, batched=True, batch_size=8)\n",
    "test_results.to_csv(\"test_results_with_generated_answers.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc89fb63",
   "metadata": {},
   "source": [
    "Use bleu and rouge scores to evaluate the entire model and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aa650577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.31\n",
      "ROUGE-1: 0.61\n",
      "ROUGE-L: 0.61\n"
     ]
    }
   ],
   "source": [
    "bleu_metric = evaluate.load(\"bleu\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "references = [[item['answer']] for item in test_results] \n",
    "predictions = [item['generated_answer'] for item in test_results] \n",
    "\n",
    "bleu_score = bleu_metric.compute(\n",
    "    predictions=predictions, \n",
    "    references=references     \n",
    ")\n",
    "print(f\"BLEU Score: {bleu_score['bleu']:.2f}\")\n",
    "\n",
    "\n",
    "rouge_score = rouge_metric.compute(\n",
    "    predictions=predictions, \n",
    "    references=[r[0] for r in references]\n",
    ")\n",
    "print(f\"ROUGE-1: {rouge_score['rouge1']:.2f}\")\n",
    "print(f\"ROUGE-L: {rouge_score['rougeL']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
